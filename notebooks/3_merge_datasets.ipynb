{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Various Datasets\n",
    "\n",
    "We have four files currently:\n",
    "1. ores_output.json -> the output from the ORES API calls giving us the quality of the articles\n",
    "2. politicians_by_country_AUG.2024.csv -> the dataset containing the politicians and their countries (Provided in the assignment)\n",
    "3. population_by_country_AUG.2024.csv -> the dataset containing the population of the countries (Provided in the assignment)\n",
    "4. wiki_information.csv -> the dataset containing rev_ids of the wiki articles\n",
    "\n",
    "In this notebook, I have documented, some of the steps to taken to format some of these files better for faster access in the analysis dataset.\n",
    "\n",
    "You can ignore this notebook if you are not interested in the details and can directly use the files present in the repository to run the 4_analysis.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the orese output file.\n",
    "\n",
    "\n",
    "### Make sure to copy the csv file from the data_files folder and place it in the path of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ores_output.json\",\"r\") as file:\n",
    "    jsonData = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the length to confirm all records are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7103"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jsonData['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting only the rev_id and prediction of the quality of the articles columns\n",
    "\n",
    "Take a careful look at the try catch block since it is important to handle the missing values in the dataset.\n",
    "\n",
    "There were 4 missing values in the dataset where there was no data or the API failed. YYou can see them in the output below.\n",
    "\n",
    "Since there is no use of these records, I have dropped them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at  1206053852\n",
      "Error in  {'httpReason': 'upstream connect error or disconnect/reset before headers. reset reason: connection failure', 'httpCode': 503}\n",
      "Error at  1150849598\n",
      "Error in  None\n",
      "Error at  1219815222\n",
      "Error in  {'httpCode': 503, 'httpReason': 'upstream connect error or disconnect/reset before headers. reset reason: connection failure'}\n",
      "Error at  1240718396\n",
      "Error in  {'httpCode': 503, 'httpReason': 'upstream connect error or disconnect/reset before headers. reset reason: connection failure'}\n"
     ]
    }
   ],
   "source": [
    "article_dict = {}\n",
    "for article in jsonData['outputs']:\n",
    "    try:\n",
    "        current_rev_id = list(article['enwiki']['scores'].keys())[0]\n",
    "        current_pred = article['enwiki']['scores'][current_rev_id]['articlequality']['score']['prediction']\n",
    "        article_dict[current_rev_id] = current_pred\n",
    "    except:\n",
    "        print(\"Error at \", current_rev_id)\n",
    "        print(\"Error in \", article)\n",
    "#jsonData['outputs'][0]['enwiki']['scores']['articlequality']['score']['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the error rate of the API calls, I have calculated the error rate and it is 0.0005 which is very low and 1% as prescribed in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005631423342249754"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(jsonData['outputs']) - len(article_dict.keys()))/len(jsonData['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of the dataset after dropping the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7099"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(article_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing these extracted values to a new file called articles_quality.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write article_dict to a new csv file\n",
    "with open(\"article_quality.csv\",\"w\") as file:\n",
    "    file.write(\"rev_id,article_quality\\n\")\n",
    "with open(\"article_quality.csv\",\"a\") as file:\n",
    "    for key in article_dict.keys():\n",
    "        file.write(\"%s,%s\\n\"%(key,article_dict[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
